{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Data | Analysis & Dummy Variables Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "\n",
    "\n",
    "2. Data Preprocessing\n",
    "    - 2.1 Importing the required packages\n",
    "    - 2.2 Loading the dataset\n",
    "    - 2.3 Preparing the data\n",
    "\n",
    "\n",
    "3. Variables Testing & Adjustments\n",
    "    - 3.1 Build a prediction model with numerical variables\n",
    "    - 3.2 Build a prediction model with numerical and categorical variables \n",
    "        - 3.2.1 Apllying dummy variables\n",
    "            - Group A \n",
    "            - Group B\n",
    "            - Applying all dummy variables\n",
    "        - 3.2.2 Convert the remaining categorical variables into numbers\n",
    "         \n",
    "         \n",
    "4. Back-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to try to improve the performance of the initial model created in the *House Prices Prediction* project, which is composed only of numerical variables.\n",
    "\n",
    "In the main notebook of the *House Prices Prediction* project we analyzed the distrubution and contribution of the categorical variables individually. Now, we let's go a step further by analyzing the best way in which categorical variables can positively impact model performance.\n",
    "\n",
    "To get it, we are going to convert into dummy those variables that have a number of options less than 5 and a high impact on the dependent variable. Then, regarding the remaining numerical variables, we will convert them into numbers and build the final model for our House Price Prediction analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing the requiered packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, boxcox_normmax, norm\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import warnings\n",
    "pd.options.display.max_columns = 250\n",
    "pd.options.display.max_rows = 250\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'df_train_clean.csv' does not exist: b'df_train_clean.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-84107cbcb65a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#loading the training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_train_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'df_train_clean.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_train_clean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'df_train_clean.csv' does not exist: b'df_train_clean.csv'"
     ]
    }
   ],
   "source": [
    "#loading the training set\n",
    "df_train_clean = pd.read_csv('df_train_clean.csv')\n",
    "df_train_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove variables with low correlation\n",
    "df_train_clean.drop(['MoSold', 'ScreenPorch', '3SsnPorch', 'PoolArea', 'MiscVal', 'YrSold', 'LowQualFinSF', 'MSSubClass',\n",
    "               'BsmtFinSF2', 'BsmtHalfBath'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shape of the dataframe after removing the variables with low correlation\n",
    "df_train_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the Dependent and Independent variables\n",
    "X_train = df_train_clean.iloc[:, :-1] #all lines, all columns except the last one\n",
    "y_train = df_train_clean.iloc[:, 64] #all lines, only the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shaape of X_train and y_train\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Variables Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Build a prediction model with numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_model_3 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_model_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_3 = LogisticRegression (random_state = 0)\n",
    "log_regressor_3.fit(pilot_model_3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_3 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_3.score(pilot_model_3, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_3 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_3.predict(pilot_model_3) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Build a prediction model with numerical and categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Note =>> We are going to analyze the following categorical variables:\n",
    "\n",
    "Street, LotShape, LandContour ,Utilities, LandSlope, BldgType, MasVnrType, ExterQual, ExterCond, LotConfig Neighborhood, Condition1, Condition2, HouseStyle, RoofStyle, RoofMatl, Exterior1st, Exterior2nd.\n",
    "\n",
    "\n",
    "Regarding the dummy variables conversion, we are only going to treat the following variables keeping in mind the analysis performed in the main House Price Prediction Notebook:\n",
    "\n",
    "*Street, LotShape, LandContour ,Utilities, LandSlope, BldgType, MasVnrType, ExterQual, ExterCond*\n",
    "\n",
    "\n",
    "The remaining categoriacal variables will be converted into numbers at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Apllying dummy variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group A\n",
    "\n",
    "In this section we are going to treat the following variables keeping in mind the analysis performed in the main House Price Prediction Notebook:\n",
    "\n",
    "*LotShape, LandContour, LandSlope, BldgType, MasVnrType, ExterQual*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1 - LandSlope__\n",
    "\n",
    "The *LandSlope* variable identifies the slope of property (pendiente de la propiedad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the LandSlope variable into dummy variables\n",
    "X_train = pd.get_dummies (X_train, columns = ['LandSlope'])\n",
    "#check the shape of df_object after converting the variables into dummy\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + LandSlope dummy variables\n",
    "pilot_model_4 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF', 'LandSlope_Gtl','LandSlope_Mod', 'LandSlope_Sev']]\n",
    "\n",
    "pilot_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_4 = LogisticRegression (random_state = 0)\n",
    "log_regressor_4.fit(pilot_model_4, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_4 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_4.score(pilot_model_4, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_4 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_4.predict(pilot_model_4) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It looks like the **Score and MSE of our model **improved remarkably** after including the 'LandSlope_Gtl','LandSlope_Mod' and 'LandSlope_Sev'variables compared to the pilot_model_3 (\"0.6223 VS 0.6102\" and \"476450343.126 VS 515970129.339\" respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2 - LotShape__\n",
    "\n",
    "The *LotShape* variable identifies the general shape of property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the LotShape variable into dummy variables\n",
    "X_train = pd.get_dummies (X_train, columns = ['LotShape'])\n",
    "#check the shape of df_object after converting the variables into dummy\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + LotShape dummy variables\n",
    "pilot_model_5 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF', 'LotShape_IR1', 'LotShape_IR2', 'LotShape_IR3', 'LotShape_Reg']]\n",
    "\n",
    "pilot_model_5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_5 = LogisticRegression (random_state = 0)\n",
    "log_regressor_5.fit(pilot_model_5, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_5 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_5.score(pilot_model_5, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_5 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_5.predict(pilot_model_5) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It looks like the **Score and MSE of our model **improved remarkably** after including the 'LotShape_IR1', 'LotShape_IR2', 'LotShape_IR3', 'LotShape_Reg' variables compared to the pilot_model_3 (\"0.6321 VS 0.6102\" and \"464727139.601 VS 515970129.339\" respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3 - LandContour__\n",
    "\n",
    "The LandContour variable identifies the flatness of the property (planitud del inmueble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the LandContour  variable into dummy variables\n",
    "X_train = pd.get_dummies (X_train, columns = ['LandContour'])\n",
    "#check the shape of df_object after converting the variables into dummy\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + LandContour dummy variables\n",
    "pilot_model_6 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF', 'LandContour_Bnk', 'LandContour_HLS', 'LandContour_Low',\n",
    "                         'LandContour_Lvl']]\n",
    "\n",
    "pilot_model_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_6 = LogisticRegression (random_state = 0)\n",
    "log_regressor_6.fit(pilot_model_6, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_6 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_6.score(pilot_model_6, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_6 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_6.predict(pilot_model_6) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It looks like the **Score and MSE of our model **improved remarkably** after including the 'LandContour_Bnk', 'LandContour_HLS', 'LandContour_Low', 'LandContour_Lvl' variables compared to the pilot_model_3 (\"0.6261 VS 0.6102\" and \"487159486.735 VS 515970129.339\" respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4 - MasVnrType__\n",
    "\n",
    "The MasVnrType variable identifies the Masonry veneer type / tipo de chapa de albañilería."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the LandContour  variable into dummy variables\n",
    "X_train = pd.get_dummies (X_train, columns = ['MasVnrType'])\n",
    "#check the shape of df_object after converting the variables into dummy\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + LandContour dummy variables\n",
    "pilot_model_7 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF', 'MasVnrType_BrkCmn', 'MasVnrType_BrkFace', 'MasVnrType_None',\n",
    "                           'MasVnrType_Stone']]\n",
    "\n",
    "pilot_model_7.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_7 = LogisticRegression (random_state = 0)\n",
    "log_regressor_7.fit(pilot_model_7, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_7 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_7.score(pilot_model_7, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_7 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_7.predict(pilot_model_7) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It looks like the **Score and MSE of our model **improved remarkably** after including the 'LandContour_Bnk', 'LandContour_HLS', 'LandContour_Low', 'LandContour_Lvl' variables compared to the pilot_model_3 (\"0.6306 VS 0.6102\" and \"469581231.557 VS 515970129.339\" respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5 - ExterQual__\n",
    "\n",
    "The ExterQual variable evaluates the quality of the material on the exterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the ExtrQual  variable into dummy variables\n",
    "X_train = pd.get_dummies (X_train, columns = ['ExterQual'])\n",
    "#check the shape of df_object after converting the variables into dummy\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + ExterQual dummy variables\n",
    "pilot_model_8 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF', 'ExterQual_Ex', 'ExterQual_Fa', 'ExterQual_Gd', 'ExterQual_TA']]\n",
    "\n",
    "pilot_model_8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_8 = LogisticRegression (random_state = 0)\n",
    "log_regressor_8.fit(pilot_model_8, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_8 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_8.score(pilot_model_8, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_8 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_8.predict(pilot_model_8) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It looks like the **Score and MSE of our model **improved remarkably** after including the 'ExterQual_Ex', 'ExterQual_Fa', 'ExterQual_Gd', 'ExterQual_TA' variables compared to the pilot_model_3 (\"0.6291 VS 0.6102\" and \"439340030.620 VS 515970129.339\" respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6 - BldgType__\n",
    "\n",
    "The BldgType variable identifies the type of dwelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the LandContour  variable into dummy variables\n",
    "X_train = pd.get_dummies (X_train, columns = ['BldgType'])\n",
    "#check the shape of df_object after converting the variables into dummy\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + ExterCond dummy variables\n",
    "pilot_model_9 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF', 'BldgType_1Fam', 'BldgType_2fmCon',\n",
    "                           'BldgType_Duplex', 'BldgType_Twnhs', 'BldgType_TwnhsE']]\n",
    "\n",
    "pilot_model_9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_9 = LogisticRegression (random_state = 0)\n",
    "log_regressor_9.fit(pilot_model_9, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_9 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_9.score(pilot_model_9, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_13 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_9.predict(pilot_model_9) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It looks like the **Score and MSE of our model improved** after including the 'BldgType_1Fam', 'BldgType_2fmCon',\n",
    "'BldgType_Duplex', 'BldgType_Twnhs' and 'BldgType_TwnhsE' variables compared to the pilot_model_3 (\"0.6178 VS 0.6102\" and \"481267784.555 VS 515970129.339\" respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to build a model applying the numercial variables and the dummy variables (Group A) which improved the score of the pilot_model_3 (the initial reference model) to test if the performance of the model improved or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + dummy variables (Group A), which improved the score of the pilot_model_3\n",
    "pilot_model_10 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF', 'LandSlope_Gtl',\n",
    "                           'LandSlope_Mod', 'LandSlope_Sev', 'LotShape_IR1', 'LotShape_IR2',\n",
    "                           'LotShape_IR3', 'LotShape_Reg', 'LandContour_Bnk', 'LandContour_HLS',\n",
    "                           'LandContour_Low', 'LandContour_Lvl', 'MasVnrType_BrkCmn',\n",
    "                           'MasVnrType_BrkFace', 'MasVnrType_None', 'MasVnrType_Stone', 'ExterQual_Ex', 'ExterQual_Fa', \n",
    "                           'ExterQual_Gd', 'ExterQual_TA', 'BldgType_1Fam', 'BldgType_2fmCon',\n",
    "                           'BldgType_Duplex', 'BldgType_Twnhs', 'BldgType_TwnhsE']]\n",
    "\n",
    "pilot_model_10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_10 = LogisticRegression (random_state = 0)\n",
    "log_regressor_10.fit(pilot_model_10, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_10 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_10.score(pilot_model_10, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_10 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_10.predict(pilot_model_10) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great result!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group B\n",
    "\n",
    "In this section we are going to treat the following variables keeping in mind the analysis performed in the main House Price Prediction Notebook:\n",
    "\n",
    "*BsmtQual, BsmtCond, BsmtExposure, CentralAir, KitchenQual, GarageFinish, and PavedDrive*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1 - BsmtQual__\n",
    "\n",
    "The BsmtQual variable evaluates the height of the basement (altura del sotano)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the BsmtQual  variable into dummy variables\n",
    "X_train = pd.get_dummies (X_train, columns = ['BsmtQual'])\n",
    "#check the shape of df_object after converting the variables into dummy\n",
    "X_train.shape\n",
    "\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + BsmtQual dummy variables\n",
    "pilot_model_11 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF', 'BsmtQual_Ex',\n",
    "                           'BsmtQual_Fa', 'BsmtQual_Gd', 'BsmtQual_TA']]\n",
    "pilot_model_11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_11 = LogisticRegression (random_state = 0)\n",
    "log_regressor_11.fit(pilot_model_11, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_11 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_11.score(pilot_model_11, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_11 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_11.predict(pilot_model_11) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It looks like the **Score and MSE of our model improved** after including the 'BsmtQual_Ex','BsmtQual_Fa', 'BsmtQual_Gd' and 'BsmtQual_TA' variables compared to the pilot_model_3 (\"0.6276 VS 0.6102\" and \"438421112.044 VS 515970129.339\" respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2 - BsmtCond__\n",
    "\n",
    "The BsmtCond variable evaluates the general condition of the basement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the BsmtCond  variable into dummy variables\n",
    "X_train = pd.get_dummies (X_train, columns = ['BsmtCond'])\n",
    "#check the shape of df_object after converting the variables into dummy\n",
    "X_train.shape\n",
    "\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + BsmtCond dummy variables\n",
    "pilot_model_12 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF', 'BsmtCond_Fa', 'BsmtCond_Gd', 'BsmtCond_Po', 'BsmtCond_TA']]\n",
    "\n",
    "pilot_model_12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_12 = LogisticRegression (random_state = 0)\n",
    "log_regressor_12.fit(pilot_model_12, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_12 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_12.score(pilot_model_12, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_12 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_12.predict(pilot_model_12) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It looks like the **Score and MSE of our model improved** after including the 'BsmtCond_Fa', 'BsmtCond_Gd', 'BsmtCond_Po' and 'BsmtCond_TA' variables compared to the pilot_model_3 (\"0.6283 VS 0.6102\" and \"472572443.84 VS 515970129.339\" respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3 - BsmtExposure__\n",
    "\n",
    "The BsmtExposure variable refers to walkout or garden level walls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the BsmtExposure variable into dummy variables\n",
    "X_train = pd.get_dummies (X_train, columns = ['BsmtExposure'])\n",
    "#check the shape of df_object after converting the variables into dummy\n",
    "X_train.shape\n",
    "\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + BsmtCond dummy variables\n",
    "pilot_model_13 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF', 'BsmtExposure_Av', 'BsmtExposure_Gd', 'BsmtExposure_Mn',\n",
    "                           'BsmtExposure_No']]\n",
    "\n",
    "pilot_model_13.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_13 = LogisticRegression (random_state = 0)\n",
    "log_regressor_13.fit(pilot_model_13, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_13 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_13.score(pilot_model_13, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_13 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_13.predict(pilot_model_13) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It looks like the **Score and MSE of our model improved** after including the 'BsmtExposure_Av', 'BsmtExposure_Gd', 'BsmtExposure_Mn' and 'BsmtExposure_No' variables compared to the pilot_model_3 (\"0.6389 VS 0.6102\" and \"467935964.311 VS 515970129.339\" respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4 - CentralAir__\n",
    "\n",
    "The CentralAir variable refers to the central air conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the BsmtExposure variable into dummy variables\n",
    "X_train = pd.get_dummies (X_train, columns = ['CentralAir'])\n",
    "#check the shape of df_object after converting the variables into dummy\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the name of the columns after converting the variables into dummy\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + CentralAir dummy variables\n",
    "pilot_model_14 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF', 'CentralAir_N','CentralAir_Y']]\n",
    "\n",
    "pilot_model_14.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_14 = LogisticRegression (random_state = 0)\n",
    "log_regressor_14.fit(pilot_model_14, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_14 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_14.score(pilot_model_14, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_14 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_14.predict(pilot_model_14) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It looks like the **Score and MSE of our model improved** of our model improved after apllying the 'CentralAir_N' and'CentralAir_Y' variables compared to the pilot_model_3 (\"0.6163 VS 0.6102\" and \"493214700.237 VS 515970129.339\" respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5 - KitchenQual__\n",
    "\n",
    "The KitchenQual variable evaluates the Kitchen quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the KitchenQual variable into dummy variables\n",
    "X_train = pd.get_dummies (X_train, columns = ['KitchenQual'])\n",
    "#check the shape of df_object after converting the variables into dummy\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the name of the columns after converting the variables into dummy\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + KitchenQual dummy variables\n",
    "pilot_model_15 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF', 'KitchenQual_Ex', 'KitchenQual_Fa', 'KitchenQual_Gd',\n",
    "                           'KitchenQual_TA']]\n",
    "\n",
    "pilot_model_15.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_15 = LogisticRegression (random_state = 0)\n",
    "log_regressor_15.fit(pilot_model_15, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_15 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_15.score(pilot_model_15, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_15 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_15.predict(pilot_model_15) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It looks like the **Score and MSE of our model improved** of our model improved after apllying the 'KitchenQual_Ex', 'KitchenQual_Fa', 'KitchenQual_Gd' and 'KitchenQual_TA' variables compared to the pilot_model_3 (\"0.6351 VS 0.6102\" and \"403595969.785 VS 515970129.339\" respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6 - GarageFinish__\n",
    "\n",
    "The GarageFinish variable refers to the interior finish of the garage (remate final del garaje)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the GarageFinish variable into dummy variables\n",
    "X_train = pd.get_dummies (X_train, columns = ['GarageFinish'])\n",
    "#check the shape of df_object after converting the variables into dummy\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the name of the columns after converting the variables into dummy\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + GarageFinish dummy variables\n",
    "pilot_model_16 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF', 'GarageFinish_Fin', 'GarageFinish_RFn', 'GarageFinish_Unf']]\n",
    "\n",
    "pilot_model_16.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_16 = LogisticRegression (random_state = 0)\n",
    "log_regressor_16.fit(pilot_model_16, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_16 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_16.score(pilot_model_16, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_16 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_16.predict(pilot_model_16) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It looks like the **Score and MSE of our model improved** of our model improved after apllying the 'GarageFinish_Fin', 'GarageFinish_RFn' and 'GarageFinish_Unf' variables compared to the pilot_model_3 (\"0.6397 VS 0.6102\" and \"498530501.830 VS 515970129.339\" respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7 - PavedDrive__\n",
    "\n",
    "The PavedDrive variable refers to the paved driveway (Calzada pavimentada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the PavedDrive variable into dummy variables\n",
    "X_train = pd.get_dummies (X_train, columns = ['PavedDrive'])\n",
    "#check the shape of df_object after converting the variables into dummy\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the name of the columns after converting the variables into dummy\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + PavedDrive dummy variables\n",
    "pilot_model_17 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF', 'PavedDrive_N', 'PavedDrive_P', 'PavedDrive_Y']]\n",
    "\n",
    "pilot_model_17.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_17 = LogisticRegression (random_state = 0)\n",
    "log_regressor_17.fit(pilot_model_17, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_17 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_17.score(pilot_model_17, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_17 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_17.predict(pilot_model_17) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It looks like the Score and MSE of our model improved of our model improved after apllying the 'PavedDrive_N', 'PavedDrive_P' and 'PavedDrive_Y' variables compared to the pilot_model_3 (\"0.6216 VS 0.6102\" and \"479070896.809 VS 515970129.339\" respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to build a model applying the numercial variables and the dummy variables (Group B) which improved the score of the pilot_model_3 (the initial reference model) to test if the performance of the model improved or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + dummy variables (Group B), which improved the score of the pilot_model_3\n",
    "pilot_model_18 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF', 'BsmtQual_Ex', 'BsmtQual_Fa', 'BsmtQual_Gd', 'BsmtQual_TA', \n",
    "                          'BsmtCond_Fa', 'BsmtCond_Gd', 'BsmtCond_Po', 'BsmtCond_TA', 'BsmtExposure_Av', 'BsmtExposure_Gd', \n",
    "                          'BsmtExposure_Mn', 'BsmtExposure_No', 'CentralAir_N', 'CentralAir_Y', 'KitchenQual_Ex', \n",
    "                          'KitchenQual_Fa', 'KitchenQual_Gd', 'KitchenQual_TA', 'GarageFinish_Fin', 'GarageFinish_RFn', \n",
    "                          'GarageFinish_Unf','PavedDrive_N', 'PavedDrive_P', 'PavedDrive_Y']]\n",
    "\n",
    "pilot_model_18.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_18 = LogisticRegression (random_state = 0)\n",
    "log_regressor_18.fit(pilot_model_18, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_18 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_18.score(pilot_model_18, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_18 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_18.predict(pilot_model_18) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great results!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying all dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + dummy variables (Group A & Group B) which improved the score of the pilot_model_3\n",
    "pilot_model_19 = X_train[['OverallQual', 'GrLivArea', '1stFlrSF', 'FullBath', 'YearBuilt', 'YearRemodAdd',\n",
    "                           'TotRmsAbvGrd', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'MasVnrArea' ,\n",
    "                           'BsmtFinSF1', 'LotFrontage', 'BsmtFullBath', 'Fireplaces', 'OpenPorchSF', 'WoodDeckSF', \n",
    "                           '2ndFlrSF', 'HalfBath','LotArea', 'BedroomAbvGr', 'OverallCond', 'KitchenAbvGr',\n",
    "                           'EnclosedPorch', 'BsmtUnfSF', 'LandSlope_Gtl','LandSlope_Mod', 'LandSlope_Sev', 'LotShape_IR1', \n",
    "                           'LotShape_IR2', 'LotShape_IR3', 'LotShape_Reg', 'LandContour_Bnk', 'LandContour_HLS',\n",
    "                           'LandContour_Low', 'LandContour_Lvl', 'MasVnrType_BrkCmn', 'MasVnrType_BrkFace', 'MasVnrType_None', \n",
    "                           'MasVnrType_Stone', 'ExterQual_Ex', 'ExterQual_Fa', 'ExterQual_Gd', 'ExterQual_TA', 'BldgType_1Fam', \n",
    "                           'BldgType_2fmCon', 'BldgType_Duplex', 'BldgType_Twnhs', 'BldgType_TwnhsE', 'BsmtQual_Ex', \n",
    "                           'BsmtQual_Fa', 'BsmtQual_Gd', 'BsmtQual_TA', 'BsmtCond_Fa', 'BsmtCond_Gd', 'BsmtCond_Po', \n",
    "                           'BsmtCond_TA', 'BsmtExposure_Av', 'BsmtExposure_Gd', 'BsmtExposure_Mn', 'BsmtExposure_No', \n",
    "                           'CentralAir_N', 'CentralAir_Y', 'KitchenQual_Ex', 'KitchenQual_Fa', 'KitchenQual_Gd', \n",
    "                           'KitchenQual_TA', 'GarageFinish_Fin', 'GarageFinish_RFn', 'GarageFinish_Unf','PavedDrive_N', \n",
    "                           'PavedDrive_P', 'PavedDrive_Y']]\n",
    "\n",
    "pilot_model_19.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_19 = LogisticRegression (random_state = 0)\n",
    "log_regressor_19.fit(pilot_model_19, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_20 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_19.score(pilot_model_19, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_20 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_19.predict(pilot_model_19) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> **We have achieved the highest result by applying the combination of those dummy variables that have a positive impact on model performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Convert the remaining categorical variables into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to convert the remaining categorical variables into numbers and check the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the rest of the categorical variables into numbers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lencoders = {}\n",
    "\n",
    "for col in X_train.select_dtypes(include=['object']).columns:\n",
    "    lencoders[col] = LabelEncoder()\n",
    "    X_train[col] = lencoders[col].fit_transform(X_train[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the datatype of X_train to review that all the variables are numbers\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Review the final data\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + all dummy variables + remaining numerical variables ()\n",
    "pilot_model_20 = X_train\n",
    "\n",
    "pilot_model_20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_20 = LogisticRegression (random_state = 0)\n",
    "log_regressor_20.fit(pilot_model_20, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (𝑅2) for the pilot_model_20 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_20.score(pilot_model_20, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_20 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_20.predict(pilot_model_20) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EL MSE HA EMPEORADO**\n",
    "\n",
    "**¿CUALES HAN SIDO LAS VARIABLES QUE LO HAN PROVOCADO?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train_modificado.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Back - up"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
