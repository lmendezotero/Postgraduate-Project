{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices Prediction\n",
    "\n",
    "## (Version for the kaggle competition) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Notebook Content **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "\n",
    "\n",
    "2. Previous Data Analysis\n",
    "    - 2.1 Importing the required packages\n",
    "    - 2.2 Loading the dataset\n",
    "    - 2.3 'SalePrice' distribution analysis\n",
    "\n",
    "\n",
    "3. Missing Data Analysis\n",
    "    - 3.1 Cleaning data for columns\n",
    "    - 3.2 Cleaning data for rows\n",
    "    - 3.3 Last Adjustments & Saving the changes\n",
    "    \n",
    "    \n",
    "4. Correlation Analysis \n",
    "     - 4.1 Introduction\n",
    "     - 4.2 First Results & Data Visualization\n",
    "     - 4.3 Top 10 variables with the highest correlation analysis\n",
    "     - 4.4 Delete variables with low correlation\n",
    "\n",
    "\n",
    "5. Categorical Data Analysis \n",
    "    - 5.1 Overview and first visualizations\n",
    "    - 5.2 Detailed analysis and adjustments\n",
    "\n",
    "\n",
    "6. Building a \"baseline\" model applying logistics regression\n",
    "   - 6.1 Preparing the data\n",
    "       - 6.1.1 Getting the Dependent and Independent variables\n",
    "       - 6.1.2 Creating new dataframes based on the data type\n",
    "\n",
    "   - 6.2 Build a model with only numerical variables\n",
    "        - 6.2.1 Pilot Model 1 (numerical variables with correlation > [+0.5 & -0.5] )\n",
    "        - 6.2.2 Pilot Model 2 (all numerical variables)\n",
    "\n",
    "   - 6.3. Build a model with numerical and categorical variables\n",
    "        - 6.3.1 Convert some categorical variables into dummy variables\n",
    "        - 6.3.2 Convert the remaining categorical variables into numbers\n",
    "        - 6.3.3 Saving the changes\n",
    "\n",
    "\n",
    "7. Building a predictive model applying Random Forest Regressor\n",
    "    - 7.1 Defining the Random Forest Regressor baseline\n",
    "        * 7.1.1 Fitting the Random Forest Regressor\n",
    "        * 7.1.2 Predicting the results\n",
    "    - 7.2 Applying K-Fold Cross-Validation technique\n",
    "    - 7.3 Applying Grid-Search technique\n",
    "        * 7.3.1 Creating a parameter grid\n",
    "        * 7.3.2 Random Search Training\n",
    "        * 7.3.3 Evaluate the Random Search\n",
    "        * 7.3.4 Initiate the grid search model\n",
    "        * 7.3.5 Fitting the grid search to the data\n",
    "        * 7.3.6 Fitting the final Random Forest Regressor Model\n",
    "        * 7.3.7 Predicting the results\n",
    "        * 7.3.8 Computing metrics for the random forest model\n",
    "        * 7.3.9 Create a submission in the Kaggle competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to **find the best fitting model for predicting the house prices of the city of Ames**, using advanced regressions techniques, such as random forest or gradient boosting.\n",
    "\n",
    "To do this, we will use a data set composed of 2930 observations and 80 variables (23 nominal, 23 ordinal, 14 discrete and 20 continous), which describes the sale of individual residential property in Ames from 2006 to 2010. The data has been provided by the Ames City Assesor's Office."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Previous Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing the requiered packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, boxcox_normmax, norm\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import warnings\n",
    "pd.options.display.max_columns = 250\n",
    "pd.options.display.max_rows = 250\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from IPython.core.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loading the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Checking the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'train.csv' does not exist: b'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bea79baaf3d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#loading the training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_train_prelim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_train_prelim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'train.csv' does not exist: b'train.csv'"
     ]
    }
   ],
   "source": [
    "#loading the training set\n",
    "df_train_prelim = pd.read_csv('train.csv')\n",
    "df_train_prelim.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shape of the data\n",
    "df_train_prelim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the type of data\n",
    "df_train_prelim.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important!__\n",
    "After checking the information of the dataset, we realised that the variable *SalePrice* is the dependent variable (the value that we want to predict with our model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Checking the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the test set\n",
    "df_test_prelim = pd.read_csv('test.csv')\n",
    "df_test_prelim.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shape of the data\n",
    "df_test_prelim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the type of data\n",
    "df_test_prelim.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 'SalePrice' distribution analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to check the SalePrice column to clearly understand the distibution of prices.\n",
    "\n",
    "We are going to analyze if the dependent variable (SalePrice) follows a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the  main statistics of the dependent variable \n",
    "df_train_prelim['SalePrice'].describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping in mind the statistics, the mean price is around 180k USD. The most expensive house is for 775k USD and the cheapest is only for 34,9k USD. In addition, the 50% quantile lies at 163k USD.\n",
    "\n",
    "Let's build a histogram to review the distribution of the house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SalePrice Histogram 1\n",
    "#without applying kernel density function\n",
    "sns.set_style('darkgrid')\n",
    "fig,ax = plt.subplots(1,1,figsize=(6,6))\n",
    "sns.distplot(df_train_prelim['SalePrice'], ax=ax, kde=False)\n",
    "ax.set_xlabel('House price, USD')\n",
    "plt.suptitle('SalePrice Histogram', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SalePrice Histogram 2\n",
    "#applying kernel density function\n",
    "sns.distplot(df_train_prelim['SalePrice'], fit=norm)\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train_prelim['SalePrice'], plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check also the Skewness and Kurtosis of the SalePrice variable in order to verify if it follows a normal distribution or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"Skewness_Kurtosis.PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SalePrice Skewness & Kurtosis\n",
    "print(\"Skewness: %f\" % df_train_prelim['SalePrice'].skew())\n",
    "print(\"Kurtosis: %f\" % df_train_prelim['SalePrice'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comments:__ After analyzing the metrics, we realized that the SalePrice does not follow a normal distribution (Gaussian distribution).\n",
    "\n",
    "Let's check how many houses have a price higher than 500000 USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_prelim.query('SalePrice > 442567.0100000005')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train_prelim.query('SalePrice > 442567.0100000005'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that **only 15 houses have a price more than 440.000 UDS**. It seems like we can drop them as outliers in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the indexes related to the outliers\n",
    "df_train_prelim.query('SalePrice > 442567.0100000005')\n",
    "id_outliers = list(df_train_prelim.query('SalePrice > 442567.0100000005')['Id'])\n",
    "print(id_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the outliers values\n",
    "df_train_prelim.drop([178, 185, 440, 527, 591, 691, 769, 798, 803, 898, 1046, 1169, 1182, 1243, 1373], inplace=True)\n",
    "\n",
    "#check the shape of the dataframe after deleting the outliers\n",
    "df_train_prelim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the main statistics and distribution of the SalePrice variable after removing the outliers values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the main statistics of the dependent variable (SalePrice) \n",
    "df_train_prelim['SalePrice'].describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram 3\n",
    "sns.distplot(df_train_prelim['SalePrice'], kde=False)\n",
    "ax.set_xlabel('House price, USD')\n",
    "plt.suptitle('SalePrice Histogram (without ouliers)', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SalePrice Histogram 4\n",
    "#applying kernel density function\n",
    "sns.distplot(df_train_prelim['SalePrice'], fit=norm)\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_train_prelim['SalePrice'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SalePrice Skewness & Kurtosis\n",
    "print(\"Skewness: %f\" % df_train_prelim['SalePrice'].skew())\n",
    "print(\"Kurtosis: %f\" % df_train_prelim['SalePrice'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comments:__\n",
    "After dropping the outliears from the df_training set, we realized that the SalePrice variables shows a distribution closer to a *normal distribution*. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note [1]:__\n",
    "We have decided to drop the outliers from the df_training directly in order to avoid deleting records with duplicate records in the Id column, since when combining the training set and test set in a single dataframe the variable Id shows duplicate values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Merging the training and test dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine two datasets (df_train and df_test) and work with the data faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the 'origin' column.\n",
    "df_train_prelim['origin']= 0\n",
    "df_test_prelim['origin']= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dataframe df_total, which is a dataframe union of df_test and df_train\n",
    "df_total = pd.concat([df_train_prelim,df_test_prelim], sort = False)\n",
    "df_total.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the shape of the data\n",
    "print('\\033[1m'+ 'The Total Set is having {0} observations for {1} characteristics.'.format(*df_total.shape)+ '\\033[0m')\n",
    "print()\n",
    "\n",
    "#and the type of data\n",
    "print('\\033[1m' +'The characteristics are of following data types:' + '\\033[0m')\n",
    "df_total.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Missing Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to review how many null values we have in the dataset in order to clean the data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how many null values there are in the dataset\n",
    "df_total.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really, __there are a total of 13905 records with nan values__. The remaining 1459 values are those values of the Y variable that do not appear in the test set (15364 - 13095 = 1459)\n",
    "\n",
    "We are going to analyze the dimension of the dataset by columns and rows to decide what columns and rows should be rejected from the dataset.\n",
    "\n",
    "First, let's check the percentage of null values per column, filtering only for those columns that have NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'None' with NaN Values\n",
    "for i in df_total.columns:\n",
    "    df_total[i].replace('None', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save SalePrice info in other DF\n",
    "\n",
    "df_total.insert(0, 'New_Id', range(1, 1 + len(df_total)))\n",
    "df_salesprice = df_total[['New_Id','SalePrice']]\n",
    "df_total.drop('SalePrice', axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salesprice.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating table with \"% missing\"\n",
    "\n",
    "column_list = []\n",
    "for i in df_total.columns:\n",
    "    my_list =[i, str(df_total[i].dtype), int(df_total[i].isnull().sum()), round(df_total[i].isnull().sum()/df_total[i].isnull().count()*100,2)]\n",
    "    column_list.append(my_list) \n",
    "\n",
    "missing = pd.DataFrame(column_list, columns=['Column', 'Type', 'NonValues', 'Percentage'])\n",
    "missing = missing[missing['NonValues']!=0]\n",
    "missing.sort_values(by='NonValues', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove columns with more than 48% missing values\n",
    "df_total.drop(list(missing[missing['Percentage']>48]['Column']), axis = 1, inplace=True)\n",
    "print(df_total.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to replace the null values.\n",
    "\n",
    "For numerical variable we will replace Nulls with the median or mean (in case median = 0), while for categorical variables we will replace mising info with the most ocurring variable info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Non Values\n",
    "\n",
    "Dict = {}\n",
    "\n",
    "columns = list(missing[missing['Percentage']<48]['Column'])\n",
    "for i in columns:\n",
    "    if (missing[missing['Column'] == i]['Type'] == 'object').any():\n",
    "        df_total[i] = df_total[i].fillna(df_total[i].value_counts().index[0])                         \n",
    "    elif df_total[i].median() == 0:\n",
    "        df_total[i] = df_total[i].replace(np.NaN, df_total[i].mean(skipna=True))\n",
    "    else: df_total[i] = df_total[i].replace(np.NaN, df_total[i].median(skipna=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how many null values there are in the dataset (after cleaning the numerical variables)\n",
    "df_total.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing the adjustments in the training set, we realized that __all the null values have been removed and the 'data for training and testing is cleaned completely'.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign SalesPrice and drop help index\n",
    "\n",
    "df_total = df_total.merge(df_salesprice, on='New_Id', how='inner')\n",
    "df_total.drop('New_Id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Splitting the data into training and test dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before spliting the X_total into training and set, we will check how looks the 'origin' column after the normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train data has origin 0, and the test data 100\n",
    "df_total['origin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training set and test set\n",
    "df_train_clean = df_total[df_total['origin'] == 0].drop('origin', axis=1)\n",
    "df_test_clean = df_total[df_total['origin'] == 100].drop('origin', axis=1)\n",
    "df_train_clean.shape, df_test_clean.shape, df_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the SalePrice variable from the test dataframe\n",
    "df_test_clean.drop(['SalePrice'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the 'Id' column from the training and test dataframes(we do not need it to build the prediction model)\n",
    "df_train_clean = df_train_clean.drop('Id', axis=1)\n",
    "df_test_clean = df_test_clean.drop('Id', axis=1)\n",
    "df_train_clean.shape, df_test_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_clean.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Saving the changes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Exporting the data to csv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the df_train and df_test data to csv to check the final data\n",
    "df_train_clean.to_csv('df_train_clean.csv', index=False)\n",
    "df_test_clean.to_csv('df_test_clean.csv', index=False)\n",
    "#export the X_train with metrics to csv to analyze the data\n",
    "df_train.describe().transpose().to_csv('df_train_clean_metrics.csv', index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the df_train and df_test data to pkl to check the final data\n",
    "df_train_clean.to_pickle(\"./df_train_clean.pkl\")\n",
    "df_test_clean.to_pickle(\"./df_test_clean.pkl\")\n",
    "#export the X_train with metrics to pkl to analyze the data\n",
    "df_train_clean.describe().transpose().to_pickle(\"./df_train_clean_metrics.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Correlation Analysis  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the process of selecting the variables that will be part of our predictive model, we will analyze the correlation of numerical variables with respect to the dependent variable to review what the data shows us and make more accurate decisions.\n",
    "\n",
    "We think that the *correlation is one of the most reliable methods when ruling out have any relation to the dependent variable of the regression prediction models*. Therefore, once the correlation is calculated, we will **delete those variables present a correlation close to zero** (between the range of +0.1 and -0.1), since they do not interfere in the variation of the price either in a negative or positive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"correlation_interpretation.PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 First Results & Data Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to check the correlation between the numerical independent variables (both float and integer) and the dependent variable (SalePrice).\n",
    "\n",
    "So, let's start our correlation analysis calculating the overall result between SalePrice and numerical variables. Then, we can split the list between float values and integer values to identify more insights in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load the training data to start the analysis \n",
    "df_train = pd.read_csv('df_train_clean.csv')\n",
    "\n",
    "#Create dtype lists\n",
    "cat_cols = list(df_train_clean.select_dtypes('object').columns)\n",
    "num_cols = list(df_train_clean.select_dtypes(include=[np.number]).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check correlation between SalePrice and numerical variables\n",
    "df_train.corr()['SalePrice'][num_cols].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comments and Observations:__\n",
    "We realized that the 'GarageCars' (size of garage in car capacity) and 'GarageArea' (size of garage in square feet) variables are the ones that shows the higher possitive correlation with the SalePrice variable, within the variables of the 'float variables' group.\n",
    "\n",
    "Regarding the *integer variables*, We realized that the 'OverallQual' (rates the overall material and finish of the house) and '1stFlrSF' (first Floor square feet) variables are the ones that shows the higher possitive correlation with the SalePrice variable, within the variables of the 'integer variables' group.\n",
    "\n",
    "Now, we are going to build a plotting headmap to visualize the summary of the variables correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plotting heatmap. \n",
    "\n",
    "plt.subplots(figsize = (30,20))\n",
    "\n",
    "# Generate a mask for the upper triangle (taken from seaborn example gallery)\n",
    "mask = np.zeros_like(df_train.corr(), dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "\n",
    "sns.heatmap(df_train.corr(), \n",
    "            cmap=sns.diverging_palette(20, 220, n=200), \n",
    "            mask = mask, \n",
    "            annot=True, \n",
    "            center = 0);\n",
    "## Give title and shape. \n",
    "plt.title(\"Heatmap of all the Train_Clean Features\", fontsize = 30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Top 10 variables with the highest correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After visualizating the correlation of all te numerical variables of the training dataset, we are going to check the performance of the top 10 variables with the highest correlation with respect to the SalePrice variable because they are the ones that have a high influence on the price of housing. \n",
    "\n",
    "In order to do it, we will create some heat maps and histogram graphs to check their relevance and understand the distribution of each of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heat Map with the top 10 variables with the highest correlation to the SalePrice feature\n",
    "k = 11 #number of variables for heatmap\n",
    "corrmat = df_train.corr()\n",
    "cols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(df_train[cols].values.T)\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.set(font_scale=1)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarized information of distribution of the top 10 variables with the SalePrice feature\n",
    "sns.set()\n",
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars','GarageArea','1stFlrSF', 'TotalBsmtSF', 'FullBath', 'YearBuilt', 'YearRemodAdd', 'TotRmsAbvGrd'] \n",
    "sns.pairplot(df_train[cols], height = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to start the distribution analysis of the top 10 variables with the most high correlation building histrograms for each of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1 - GarageYrBlt__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GarageYrBlt Histogram \n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,6))\n",
    "sns.distplot(df_train['GarageYrBlt'], ax=ax, kde=False)\n",
    "\n",
    "ax.set_xlabel('')\n",
    "plt.suptitle('Garage Year Built Distribution', size=15)\n",
    "ax.set_xlabel('Years')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2 - GarageCars__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GarageCars Histogram\n",
    "df_train['GarageCars'].hist(density=0, histtype='stepfilled', bins=30)\n",
    "plt.ylabel('frequency')\n",
    "plt.xlabel('values')\n",
    "plt.title('Garage Cars Distribution', fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GarageCars relateive frequency table\n",
    "100 * df_train['GarageCars'].value_counts() / len(df_train['GarageCars'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3 - GarageArea__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GarageArea Histogram\n",
    "df_train['GarageArea'].hist(density=0, histtype='stepfilled', bins=30)\n",
    "plt.ylabel('frequency')\n",
    "plt.xlabel('values')\n",
    "plt.title('Garage Area Distribution', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4 - TotalBsmtSF__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TotalBsmtSF Histogram\n",
    "df_train['TotalBsmtSF'].hist(density=1, histtype='stepfilled', bins=30)\n",
    "plt.ylabel('frequency')\n",
    "plt.xlabel('values')\n",
    "plt.title('Total square feet of basement area - Distribution', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comments and Observations:__\n",
    "\n",
    "* Except \"Yearbuilt\", the rest of the variables show an abnormal distribution with a strong influence of the values positioned on the left side of the graph.\n",
    "* GarageCars analysis =>> About 60% of the sample analyzed has a space available for 2 cars, while the remaining 40% is divided into 1 and 3 slots for cars (27% for 1 car slot and 13% for 3 car slots, respectively).\n",
    "* Regarding the Garage Area distribution, we observe that the garages with a capacity between 400-600 square feet are those that present a higher frequency in the analyzed sample, while the total square feet of basement with a higher frequency is between the 500 and 1500 square of feet.\n",
    "* Finally, we observe that the most part of the garages built between the range of the year 1960 until the first decade of the 2000s, highlighting the high peak recorded in the first decade of the 21st century."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue with the analysis of the distribution of the numerical variables with highly correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5 - OverallQual__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OverallQual Histogram \n",
    "df_train['OverallQual'].hist(density=0, histtype='stepfilled', bins=30)\n",
    "plt.ylabel('frequency')\n",
    "plt.xlabel('values')\n",
    "plt.title('Rates of material quality - Distribution', fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OverallQual relateive frequency table\n",
    "100 * df_train['OverallQual'].value_counts() / len(df_train['OverallQual'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5 - GrLivArea__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GrLivArea Histogram \n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,6))\n",
    "sns.distplot(df_train['GrLivArea'], ax=ax, kde=False)\n",
    "\n",
    "ax.set_xlabel('Values')\n",
    "plt.suptitle('Above grade/ground living area square feet - Distribution', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6 - 1stFlrSF__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1stFlrSF Histogram \n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,6))\n",
    "sns.distplot(df_train['1stFlrSF'], ax=ax, kde=False)\n",
    "\n",
    "ax.set_xlabel('Values')\n",
    "plt.suptitle('First Floor square feet - Histogram', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7 - FullBath__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FullBath Histogram \n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,6))\n",
    "sns.distplot(df_train['FullBath'], ax=ax, kde=False)\n",
    "\n",
    "ax.set_xlabel('Full bathrooms above grade - values')\n",
    "plt.suptitle('Full bathrooms above grade - Histogram 1', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FullBath relateive frequency table\n",
    "100 * df_train['FullBath'].value_counts() / len(df_train['FullBath'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__8 - YearBuilt__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YearBuilt Histogram\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,6))\n",
    "sns.distplot(df_train['YearBuilt'], ax=ax, kde=False)\n",
    "\n",
    "ax.set_xlabel('Year Built - Values')\n",
    "plt.suptitle('Year Built - Histogram', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__9 - YearRemodAdd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#YearRemodAdd Histogram\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,6))\n",
    "sns.distplot(df_train['YearRemodAdd'], ax=ax, kde=False)\n",
    "\n",
    "ax.set_xlabel('Years')\n",
    "plt.suptitle('Remodel date - Histogram', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__10 - TotRmsAbvGrd__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TotRmsAbvGrd Histogram \n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,6))\n",
    "sns.distplot(df_train['TotRmsAbvGrd'], ax=ax, kde=False)\n",
    "\n",
    "ax.set_xlabel('Total rooms above grade - values')\n",
    "plt.suptitle('Total rooms above grade - Histogram', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TotRmsAbvGrd relateive frequency table\n",
    "100 * df_train['TotRmsAbvGrd'].value_counts() / len(df_train['TotRmsAbvGrd'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comments and Observations:__\n",
    "\n",
    "* \"Rates of material quality\" analysis =>> The 77% of the analyzed sample is located in ratios of 5 (Average), 6 (Above average) and 7 (Good). Barely the 12% has registered ratios of 8 (very good) and 9 (Excellent).\n",
    "* Regarding the capacity, checking the TotRmsAbvGrd we realized that the 52% of the sample is located along records of 6 and 7 rooms (28.62% and 23.39% respectively), while the 54% of the records of the variable FullBath is located in two-bathroom houses and the 44% in one-bathroom houses.\n",
    "* Keeping in mind the time, the construction of most of the houses was done between the 1960s and 2010, highlighting a strong peak of construction in the first decade of the 2000s. These resulst are similar to the Remodel data, so the most of the houses have been remodeled between the 1950s and 2010, highlighting a peak in the 2010s.\n",
    "* Finally, we realized that the most of the recordas of the \"First Floor square feet\" variable is between the 500 and 1500 quare feet, while the \"Above grade/ground living area square feet\" show a high record between the 1000 and 2000 square feet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Deleting variables with low correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finalize the correlation analysis, we are going to remove those numerical variables that have a correlation equal or close to 0 (between the range +0.10 and -0.10), becuase they do not show a relation with respect to the SalePrice varialbe.\n",
    "\n",
    "The numerical variables susceptible to removal are as follows:\n",
    "\n",
    "* ScreenPorch (0.084846)\n",
    "* MoSold (0.073477)\n",
    "* 3SsnPorch (0.057596)\n",
    "* PoolArea (0.034475)\n",
    "* BsmtFinSF2 (-0.009578)\n",
    "* MiscVal (-0.019445)\n",
    "* BsmtHalfBath (-0.030081)\n",
    "* YrSold (-0.033782)\n",
    "* LowQualFinSF (-0.060264)\n",
    "* OverallCond (-0.076713)\n",
    "* MSSubClass (-0.085869)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove variables with low correlation\n",
    "df_train.drop(['MoSold', 'ScreenPorch', '3SsnPorch', 'PoolArea', 'MiscVal', 'YrSold', 'LowQualFinSF', 'MSSubClass',\n",
    "               'BsmtFinSF2', 'BsmtHalfBath', 'OverallCond'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shape of the dataframe after removing the variables with low correlation\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the columns after removing the variables with low correlation\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we have to implement in the test set the same changes we made earlier in the training set in order to have the inforamtion in the same format and get realistic results.\n",
    "\n",
    "Therefore, we proceed to remove those variables that show a low correlation with respect to the SalePrice variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test data to implement the changes\n",
    "df_test = pd.read_csv('df_test_clean.csv') \n",
    "\n",
    "#remove variables with low correlation\n",
    "df_test.drop(['MoSold', 'ScreenPorch', '3SsnPorch', 'PoolArea', 'MiscVal', 'YrSold', 'LowQualFinSF', 'MSSubClass',\n",
    "               'BsmtFinSF2', 'BsmtHalfBath', 'OverallCond'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shape of the test set after removing variables with low correlation\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data to csv\n",
    "df_train.to_csv('df_train_corr.csv', index=False)\n",
    "df_test.to_csv('df_test_corr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Categorical Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain clean data for building the prediction models, in the previous sections, we have analyzed the data sets considering only the numerical features. In this section, we will analyze the behavior of the categorical values and their impact on predicted value. To this end, we will start plotting the categorical values and then we will analyze their distribution in order to assess if they can potentially be discarded, or if need to be analyzed deeply. If the analyzed categorical feature presents a uniform distribution we have established their possible discarding (i.e. all categories with the same or similar values), otherwise the feature must be analyzed individually considering even the inclusion of dummy variables if the feature has few categories (at most 4) in order to enrich the data set and the accuracy of the future prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Overview and first visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Count plots for categorical variables\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=0, figsize =(20,240))\n",
    "sns.color_palette(\"husl\", 8)\n",
    "plt.subplots_adjust(right=2)\n",
    "plt.subplots_adjust(top=2)\n",
    "\n",
    "for i, feature in enumerate(cat_cols, 1):\n",
    "    plt.subplot(len(cat_cols), 3,i)\n",
    "    sns.countplot(df_train[feature])\n",
    "    plt.xlabel(f'{feature}', size=20)\n",
    "    plt.ylabel('Count', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Categorised Mean/Median for categorical data\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=0, figsize =(12,120))\n",
    "sns.color_palette(\"husl\", 8)\n",
    "plt.subplots_adjust(right=2)\n",
    "plt.subplots_adjust(top=2)\n",
    "\n",
    "for i, feature in enumerate(cat_cols, 1):\n",
    "    plt.subplot(len(cat_cols), 3,i)\n",
    "    df_train.groupby(feature)['SalePrice'].median().plot.bar()\n",
    "    plt.xlabel(f'{feature}', size=15)\n",
    "    plt.ylabel('Count', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comments and Observatons:__\n",
    "\n",
    "\n",
    "It seems that we find a high volume of categorical variables, which have a very dispersed data distribution, so it is difficult to quantify whether they can all provide the same value when constructing the model.\n",
    "\n",
    "Therefore, in order to analyze which are the most impactful categorical variables, we will analyze them one by one and evaluate whether they are worth converting them into dummy variables to build a robust predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Detailed analysis and adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will analyze each of the variables to see what information they provide us and check if it can add value to the model or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a) MSZoning__\n",
    "\n",
    "The variable MSZoning identifies the general zoning classification of the sale (agriculture, commercial, residential, etc).\n",
    "\n",
    "* A\tAgriculture\n",
    "* C\tCommercial\n",
    "* FV\tFloating Village Residential\n",
    "* I\tIndustrial\n",
    "* RH\tResidential High Density\n",
    "* RL\tResidential Low Density\n",
    "* RP\tResidential Low Density Park \n",
    "* RM\tResidential Medium Density\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSZoning bar chart\n",
    "plot = df_train['MSZoning'].value_counts().plot(kind='bar', color=['tab:blue','tab:orange','tab:green'], title='MSZoning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b) Street__\n",
    "\n",
    "The variable Street identifies the type of road access to property (gravael or paved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Street bar chart\n",
    "plot = df_train['Street'].value_counts().plot(kind='bar', \n",
    "                                                      color=['tab:blue','tab:orange','tab:green'], title='Street - type of road access to the property')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It could be interesting to convert the Street variable into dummy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c) LotShape__\n",
    "\n",
    "The variable LotShape identifies the general shape of property.\n",
    "\n",
    "* Reg\t- Regular\t\n",
    "* IR1\t- Slightly irregular\n",
    "* IR2\t- Moderately Irregular\n",
    "* IR3\t- Irregular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LotShape bar chart\n",
    "plot = df_train['LotShape'].value_counts().plot(kind='bar', color=['tab:blue','tab:orange','tab:green'], \n",
    "                                                title='General shape of property')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It could be interesting to convert the Street variable into dummy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d) LandContour__\n",
    "\n",
    "The variable LandContour identifies the flatness of the property.\n",
    "\n",
    "* Lvl\t- Near Flat/Level\t\n",
    "* Bnk\t- Banked - Quick and significant rise from street grade to building\n",
    "* HLS\t- Hillside - Significant slope from side to side\n",
    "* Low\t- Depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LandContour bar chart\n",
    "plot = df_train['LandContour'].value_counts().plot(kind='bar', color=['tab:blue','tab:orange','tab:green'],\n",
    "                                                   title='Flatness of the property')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It could be interesting to convert the Street variable into dummy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__e) Utilities__\n",
    "\n",
    "The variable Utilities identifies the type of utilities available (all public utilities, electricity, gas, water, etc).\n",
    "\n",
    "* AllPub\t-- All public Utilities (E,G,W,& S)\t\n",
    "* NoSewr\t-- Electricity, Gas, and Water (Septic Tank)\n",
    "* NoSeWa\t-- Electricity and Gas Only\n",
    "* ELO\t-- Electricity only\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities bar chart\n",
    "plot = df_train['Utilities'].value_counts().plot(kind='bar', \n",
    "                                                      color=['tab:blue','tab:orange','tab:green'], title='Type of utilities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It could be interesting to convert the Utilities variable into dummy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__f) LotConfig__\n",
    "\n",
    "The variable LotConfig identifies the lot configuration.\n",
    "\n",
    "* Inside\t-- Inside lot\n",
    "* Corner\t-- Corner lot\n",
    "* CulDSac\t-- Cul-de-sac\n",
    "* FR2\t-- Frontage on 2 sides of property\n",
    "* FR3\t-- Frontage on 3 sides of property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities bar chart\n",
    "plot = df_train['LotConfig'].value_counts().plot(kind='bar', \n",
    "                                                      color=['tab:blue','tab:orange','tab:green'], title='Lot Configuration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration =>> I am not sure if it has sense to include this variable or not (I could not understand the meaning of it) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__g) LandSlope__\n",
    "\n",
    "The variable LandSlope identifies the slope of property.\n",
    "\n",
    "* Gtl\t-- Gentle slope\n",
    "* Mod\t-- Moderate Slope\t\n",
    "* Sev\t-- Severe Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LandSlope bar chart\n",
    "plot = df_train['LandSlope'].value_counts().plot(kind='bar', \n",
    "                                                      color=['tab:blue','tab:orange','tab:green'], title='LandSlope')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It could be interesting to convert the LandSlope variable into dummy and work together with the variable LandContour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__h) Neighborhood__\n",
    "\n",
    "The variable Neighborhood identifies the physical locations within Ames city limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LandSlope bar chart\n",
    "plot = df_train['Neighborhood'].value_counts().plot(kind='bar', color=['tab:blue','tab:orange','tab:green'], \n",
    "                                                    title='Physical locations within Ames city limits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> I am not sure if it has sense to include this variable or not in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__i) Condition1__\n",
    "\n",
    "The variable Condition1 identifies the proximity to various conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition1 bar chart\n",
    "plot = df_train['Condition1'].value_counts().plot(kind='bar', \n",
    "                                                      color=['tab:blue','tab:orange','tab:green'], title='Condition 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition2 relateive frequency table\n",
    "100 * df_train['Condition1'].value_counts() / len(df_train['Condition1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__j) Condition2__\n",
    "\n",
    "The variable Condition2 identifies the proximity to various conditions (if more than one is present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition1 bar chart\n",
    "plot = df_train['Condition2'].value_counts().plot(kind='bar', \n",
    "                                                      color=['tab:blue','tab:orange','tab:green'], title='Condition 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition2 relateive frequency table\n",
    "100 * df_train['Condition2'].value_counts() / len(df_train['Condition2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__k) BldgType__\n",
    "\n",
    "The variable BldgType identifies the type of dwelling/housing.\n",
    "\n",
    "* 1Fam\t-- Single-family Detached\t\n",
    "* 2FmCon\t-- Two-family Conversion; originally built as one-family dwelling\n",
    "* Duplx\t-- Duplex\n",
    "* TwnhsE\t-- Townhouse End Unit\n",
    "* TwnhsI\t-- Townhouse Inside Unit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BldgType bar chart\n",
    "plot = df_train['BldgType'].value_counts().plot(kind='bar', \n",
    "                                                      color=['tab:blue','tab:orange','tab:green'], title='BldgType')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It could be interesting to convert the BldgType variable into dummy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__l) HouseStyle__\n",
    "\n",
    "The variable BldgType identifies the style of dwelling/housing.\n",
    "\n",
    "* 1Story\tOne story\n",
    "* 1.5Fin\tOne and one-half story: 2nd level finished\n",
    "* 1.5Unf\tOne and one-half story: 2nd level unfinished\n",
    "* 2Story\tTwo story\n",
    "* 2.5Fin\tTwo and one-half story: 2nd level finished\n",
    "* 2.5Unf\tTwo and one-half story: 2nd level unfinished\n",
    "* SFoyer\tSplit Foyer\n",
    "* SLvl\tSplit Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BldgType bar chart\n",
    "plot = df_train['HouseStyle'].value_counts().plot(kind='bar', \n",
    "                                                      color=['tab:blue','tab:orange','tab:green'], title='HouseStyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__m) RoofStyle__\n",
    "\n",
    "The variable RoofStyle identifies the type of roof.\n",
    "\n",
    "* Flat\t-- Flat\n",
    "* Gable\t-- Gable\n",
    "* Gambrel\t-- Gabrel (Barn)\n",
    "* Hip\t-- Hip\n",
    "* Mansard\t-- Mansard\n",
    "* Shed\t-- Shed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoofStyle bar chart\n",
    "plot = df_train['RoofStyle'].value_counts().plot(kind='bar', \n",
    "                                                      color=['tab:blue','tab:orange','tab:green'], title='Roof Style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoofStyle relateive frequency table\n",
    "100 * df_train['RoofStyle'].value_counts() / len(df_train['RoofStyle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__n) RoofMatl:__\n",
    "\n",
    "The variable RoofMatl:  identifies the roof material.\n",
    "\n",
    "* ClyTile\tClay or Tile\n",
    "* CompShg\tStandard (Composite) Shingle\n",
    "* Membran\tMembrane\n",
    "* Metal\tMetal\n",
    "* Roll\tRoll\n",
    "* Tar&Grv\tGravel & Tar\n",
    "* WdShake\tWood Shakes\n",
    "* WdShngl\tWood Shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoofStyle bar chart\n",
    "plot = df_train['RoofMatl'].value_counts().plot(kind='bar', \n",
    "                                                      color=['tab:blue','tab:orange','tab:green'], title='Roof Material')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoofMatl relateive frequency table\n",
    "100 * df_train['RoofMatl'].value_counts() / len(df_train['RoofMatl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__o) Exterior1st:__\n",
    "\n",
    "The variable Exterior1st identifies the exterior covering on house.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoofStyle bar chart\n",
    "plot = df_train['Exterior1st'].value_counts().plot(kind='bar', \n",
    "                                                      color=['tab:blue','tab:orange','tab:green'], \n",
    "                                                         title='Exterior covering on house 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__p) Exterior2nd:__\n",
    "\n",
    "The variable Exterior2nd identifies the exterior covering on house (if more than one material)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoofStyle bar chart\n",
    "plot = df_train['Exterior2nd'].value_counts().plot(kind='bar', \n",
    "                                                      color=['tab:blue','tab:orange','tab:green'], \n",
    "                                                         title='Exterior covering on house 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__q) ExterQual:__\n",
    "\n",
    "The variable ExterQual evaluates the quality of the material on the exterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExterQual bar chart\n",
    "plot = df_train['ExterQual'].value_counts().plot(kind='bar', \n",
    "                                                      color=['tab:blue','tab:orange','tab:green'], \n",
    "                                                    title='Quality of the material on the exterior')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> I am not sure if it has sense to include this variable or not in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__r) ExterCond:__\n",
    "\n",
    "The variable ExterCond evaluates the present condition of the material on the exterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExterCond bar chart\n",
    "plot = df_train['ExterCond'].value_counts().plot(kind='bar', \n",
    "                                                      color=['tab:blue','tab:orange','tab:green'], \n",
    "                                                    title='Present condition of the material on the exterior')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> I am not sure if it has sense to include this variable or not in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Summary of variables that could be transformed into dummy due to  its characteristics:__\n",
    "\n",
    "* Street (2 options)\n",
    "* LotShape (4 options)\n",
    "* LandContour (4 options)\n",
    "* ExtrQual (4 options)\n",
    "* ExtCond (4 options)\n",
    "* Utilities (2 options)\n",
    "* LandSlope (3 options)\n",
    "* BldgType (5 options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Building a baseline model applying Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1 Getting the Dependent and Independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the Dependent and Independent variables\n",
    "X_train = df_train.iloc[:, :-1] #all lines, all columns except the last one\n",
    "y_train = df_train.iloc[:, 62] #all lines, only the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shaape of X_train and y_train\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2 Creating new dataframes based on the data type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the creation of our prediction model builing some dataframes related to the datatype of the variables that are part of X_train. These dataframes will help us to build a pilot model composed of only numerical variables.\n",
    "\n",
    "Then, we are going to add categorical variables to our model to improve the score and the power prediction of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create dtype dataframes\n",
    "#create a dataframe with only categorical variables\n",
    "df_object = X_train.select_dtypes(include=[object])\n",
    "#create a dataframe with only numerical variables\n",
    "df_number = X_train.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_object.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_object.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the columns of the df_object dataframe\n",
    "df_object.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> We realized that we have a total of 38 numerical variables. For our predictive model, we have to convert them to numerics in order to be able to apply algorithms such as logistics regression and random forest. However, we can not transform them all at once, as in some cases it is convenient to turn some variables into dummy to achieve a positive impact on the total set of the model.\n",
    "\n",
    "In the following section, you will find more details about the categorical varaibles treatment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_number.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_number.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the columns of the df_number dataframe\n",
    "df_number.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments => We realized that we have a total of 26 numerical variables. For our predictive model, we can we consider all the numerical variables, since both Logistics Regression and Random Forest operate with numerical variables and the volume is not very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Build a model with only numerical variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Pilot Model 1 (numerical variables with correlation > [+0.5 & -0.5] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start creating a pilot model only with those variables that have a higher correlation with respect to the dependent variable (more than 0.50 of correlation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_model_1 = df_number[['OverallQual','GrLivArea', 'GarageCars', 'GarageArea','TotalBsmtSF', '1stFlrSF', 'YearBuilt', \n",
    "                           'FullBath','YearRemodAdd' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_model_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_1 = LogisticRegression (random_state = 0)\n",
    "log_regressor_1.fit(pilot_model_1, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important Note:__ The classifier learns the correlation between the df_number and the x_train. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start calculating the $R^2$ (coefficient of determination) regression score function, which determines the quality of the model to replicate the results and the proportion of variation of the results that can be explained by the model.\n",
    "\n",
    "**Best possible score is 1.0 and it can be negative** (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a $R^2$ score of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (2) for the pilot_model_1 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_1.score(pilot_model_1, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_1 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_1.predict(pilot_model_1) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 Pilot Model 2 (all numerical variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to include all the numerical varaibles into the pilot model in order to check its performance and define a preliminary baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_model_2 = df_number[['OverallQual','GrLivArea', 'GarageCars', 'GarageArea','TotalBsmtSF', '1stFlrSF', 'YearBuilt', \n",
    "                           'FullBath','YearRemodAdd','TotRmsAbvGrd','GarageYrBlt','Fireplaces','MasVnrArea','BsmtFinSF1',\n",
    "                           'WoodDeckSF', 'LotFrontage','OpenPorchSF', '2ndFlrSF','HalfBath','LotArea','BsmtFullBath',  \n",
    "                            'BsmtUnfSF','BedroomAbvGr','EnclosedPorch','KitchenAbvGr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_model_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_2 = LogisticRegression (random_state = 0)\n",
    "log_regressor_2.fit(pilot_model_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (2) for the pilot_model_2 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_2.score(pilot_model_2, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_2 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_2.predict(pilot_model_2) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comments:__ We realized that the Score and the MSE have improved after includding all numerical variables, which present a significant level of correlation with respect to the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note [2]:__ We tried to improve the performance of the model checking the distribution of all the numerical variables that are part of the model to see what kind of distribution they showed and adjusting the distribution of those variables that followed a distribution close to the Gaussian by applying logarithms. \n",
    "\n",
    "However, we realized that the score of the Logistics Regression worsened after applying logaritm functions, so we decided  not to use this method to improve model performance, as the results are not as expected.\n",
    "\n",
    "You can check the details of our analysis clicking in the following link:\n",
    "\n",
    "[Notebook - Testing Variables Distribution Applying 'Logarithms'](https://github.com/lmendezotero/Postgraduate-Project/blob/master/House%20Prices%20Prediction/Testing%20Variables%20Distribution%20Applying%20'Logarithms'.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Build a model with numerical and categorical variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.1 Convert some categorical variables into dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to build our predictive model keeping in mind both numerical and categorical variables is to convert into dummy those categorical variables that have a positive impact on the model and that have few options/classes in order to avoid a significantly increase the number of features in the dataset. \n",
    "\n",
    "We do not want to make this notebook too extensive and include the necessary code without falling into redundancies. So, we have already done this analysis in other notebooks, which are linked to this current file.\n",
    "\n",
    "In the following Jupyter Notebook you can see the analysis performed in which we converted into dummy variables all the variables that seemed to us subject to being converted and the impact of each variable on the performance of the model: \n",
    "\n",
    "[Categorical Data - Dummy Variables Testing](https://github.com/lmendezotero/Postgraduate-Project/blob/master/House%20Prices%20Prediction/Categorical%20Data%20-%20Dummy%20Variables%20Testing.ipynb)\n",
    "\n",
    "However, we found that certain variables ('ExterCond', 'Utilities' and 'Street') that were transformed to dummy did not provide a positive impact on the model. So, we have to exclude these variables from the dummy analysis and we created a final version of the Notebook, in which we have tested the performance of the model with all the choosen dummy variables and the remaining categorical variables converted into numbers.\n",
    "\n",
    "You can check the details of our analysis in the Jupyter Notebook:\n",
    "\n",
    "[Categorical Variables - Analysis & Testing.ipynb](https://github.com/lmendezotero/Postgraduate-Project/blob/master/House%20Prices%20Prediction/Categorical%20Data%20-%20Analysis%20%26%20Testing.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's go!\n",
    "\n",
    "We are going to start building our predictive model converting the choosen categorical variables into dummy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the choosen categorical variables into dummy variables\n",
    "X_train = pd.get_dummies (X_train, columns = ['LotShape', 'LandContour', 'LandSlope', 'BldgType', 'ExterQual', \n",
    "                                              'BsmtQual', 'BsmtCond', 'BsmtExposure', 'CentralAir', 'KitchenQual', \n",
    "                                              'GarageFinish', 'PavedDrive'])\n",
    "\n",
    "#check the shape of df_object after converting the variables into dummy\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the name of the columns after converting the variables into dummy\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we proceed to merge all the dummy variables in the same pilot_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#numerical model + dummy variables\n",
    "pilot_model_3 = X_train[['OverallQual','GrLivArea', 'GarageCars', 'GarageArea','TotalBsmtSF', '1stFlrSF', 'YearBuilt', \n",
    "                           'FullBath','YearRemodAdd','TotRmsAbvGrd','GarageYrBlt','Fireplaces','MasVnrArea','BsmtFinSF1',\n",
    "                           'WoodDeckSF', 'LotFrontage','OpenPorchSF', '2ndFlrSF','HalfBath','LotArea','BsmtFullBath',  \n",
    "                            'BsmtUnfSF','BedroomAbvGr','EnclosedPorch','KitchenAbvGr','LotShape_IR1', 'LotShape_IR2',\n",
    "                           'LotShape_IR3', 'LotShape_Reg', 'LandContour_Bnk','LandContour_HLS', 'LandContour_Low',\n",
    "                          'LandContour_Lvl','LandSlope_Gtl', 'LandSlope_Mod', 'LandSlope_Sev', 'BldgType_1Fam',\n",
    "                           'BldgType_2fmCon', 'BldgType_Duplex', 'BldgType_Twnhs','BldgType_TwnhsE', 'ExterQual_Ex', \n",
    "                           'ExterQual_Fa', 'ExterQual_Gd','ExterQual_TA', 'BsmtQual_Ex', 'BsmtQual_Fa', 'BsmtQual_Gd',\n",
    "                           'BsmtQual_TA', 'BsmtCond_Fa', 'BsmtCond_Gd', 'BsmtCond_Po', 'BsmtCond_TA', 'BsmtExposure_Av',\n",
    "                           'BsmtExposure_Gd', 'BsmtExposure_Mn','BsmtExposure_No', 'CentralAir_N', 'CentralAir_Y', \n",
    "                         'KitchenQual_Ex','KitchenQual_Fa', 'KitchenQual_Gd', 'KitchenQual_TA', 'GarageFinish_Fin',\n",
    "                           'GarageFinish_RFn', 'GarageFinish_Unf','PavedDrive_N', 'PavedDrive_P', 'PavedDrive_Y']]\n",
    "\n",
    "pilot_model_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fitting the X_training applying logistics regression to check the performance of the model after includding all the dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_3 = LogisticRegression (random_state = 0)\n",
    "log_regressor_3.fit(pilot_model_3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (2) for the pilot_model_3 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_3.score(pilot_model_3, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_3 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_3.predict(pilot_model_3) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> It looks like the Score and MSE of our model improved improved after apllying all the dummy variables compared to the pilot_model_2 (\"0.7328 VS 0.5723\" and \"254.689.611 VS 598.234.875\" respectively).\n",
    "\n",
    "So, the *combination of those dummy variables and the numerical variables have raised a positive impact on model performance*.\n",
    "\n",
    "\n",
    "Now, we procced to implement the same changes in the X_test dataframe to validate the data in the same uniform format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert some categorical variables into dummy variables\n",
    "df_test = pd.get_dummies (df_test, columns = ['LotShape', 'LandContour', 'LandSlope', 'BldgType', 'ExterQual', \n",
    "                                              'BsmtQual', 'BsmtCond', 'BsmtExposure', 'CentralAir', 'KitchenQual', \n",
    "                                              'GarageFinish', 'PavedDrive'])\n",
    "\n",
    "#check the shape of test set after converting the variables into dummy\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.2 Convert the remaining categorical variables into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to convert the remaining categorical variables into numbers and check the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the rest of the categorical variables into numbers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lencoders = {}\n",
    "\n",
    "for col in X_train.select_dtypes(include=['object']).columns:\n",
    "    lencoders[col] = LabelEncoder()\n",
    "    X_train[col] = lencoders[col].fit_transform(X_train[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the datatype of X_train to review that all the variables are numbers\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge all the variables in the same pilot_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical model + all dummy variables + remaining numerical variables\n",
    "pilot_model_4 = X_train\n",
    "\n",
    "pilot_model_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting logistic Regression into the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_regressor_4 = LogisticRegression (random_state = 0)\n",
    "log_regressor_4.fit(pilot_model_4, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (2) for the pilot_model_4 and y_training\n",
    "print('Training Score: {}'.format(log_regressor_4.score(pilot_model_4, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the pilot_model_4 and y_training\n",
    "print('Training MSE: {}'.format(np.mean((log_regressor_4.predict(pilot_model_4) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> Once we apply logistics regression with all the transformed variables, we observe that we achieve a Score of 0.8747 and a Mean Squared Error of result of 114.541.500, respectively.\n",
    "\n",
    "Therefore, consider the pilot_model_4 as the reference or baseline. So, the goal is to improve the Score and the MSE or our model applying one of the most famous ensemble algorithm that is called the Random Forest Regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before saving the changes, we have to perform the same actions in the X_test dataframe to validate the data in the same uniform format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the rest of the categorical variables into numbers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lencoders = {}\n",
    "\n",
    "for col in df_test.select_dtypes(include=['object']).columns:\n",
    "    lencoders[col] = LabelEncoder()\n",
    "    df_test[col] = lencoders[col].fit_transform(df_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the datatype of X_train to review that all the variables are numbers\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Review the final data\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to training the Random Forest Regressor model, as the training set and test set are aligned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.3 Saving the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's check the shape of the final training dataframes\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the baseline model to csv\n",
    "X_train.to_csv('Xtrain_baseline_model.csv', index=False)\n",
    "y_train.to_csv('ytrain_baseline_model.csv', index = False)\n",
    "df_test.to_csv('Xtest_baseline_model.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the baseline model to pickle\n",
    "X_train.to_pickle(\"./Xtrain_baseline_model.pkl\")\n",
    "y_train.to_pickle(\"./y_training.pkl\")\n",
    "df_test.to_pickle(\"./Xtest_baseline_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Computing metrics for the logistics regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the test results of the train set\n",
    "y_pred_1 = log_regressor_4.predict(X_train)\n",
    "np.set_printoptions(precision=2)\n",
    "print(y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "print(\"R-squared =\", metrics.r2_score(y_train, y_pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE =\", metrics.mean_squared_error(y_train, y_pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the test results of the test set\n",
    "y_pred_2 = log_regressor_4.predict(df_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print(y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2_df = pd.DataFrame(y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1_df = pd.DataFrame(y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv\n",
    "y_pred_1_df.to_csv('y_pred_logreg_train.csv', index=False)\n",
    "y_pred_2_df.to_csv('y_pred_logreg_test.csv', index=False)                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Building a predictive model applying Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Defining the Random Forest Regressor baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start our analysis building a simple random forest regressor model, which is the baseline. Then, applying cross-validation techniques we will search what are the best parameters and we will apply them in order to build a solid and robust predictive model.\n",
    "\n",
    "So, let's start creating the Random Forest baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.1 Fitting the Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Random Forest Regression to the dataset\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rf_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (2) for the X_train and y_training\n",
    "print('Training Score: {}'.format(rf_regressor.score(X_train, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the X_train  and y_training\n",
    "print('Training MSE: {}'.format(np.mean((rf_regressor.predict(X_train) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the results obtained in the random forest model compared to logistics regression, we verify that the random forest shows a greater potential, as we have increased the score by about 10% (from 0.87 to 0.97).\n",
    "\n",
    "Therefore, we define a Random Forest baseline model that has a Score of 0.974 and a MSE of 126.354.326,35. So, **our goal is to try to improve these results.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2 Predicting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set results\n",
    "y_pred = rf_regressor.predict(df_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments => As we do not have any test labels (y_test) to validate the predictions of our model, we have to look for other method to corroborate that our model is trained properly (without falling into overfitting) and predicts new results correctly.\n",
    "\n",
    "One possible option could be applying **Cross-Validation techniques**, through which we are going to work and validate directly the data based on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Applying K-Fold Cross-Validation technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common cross-validation techniques is the **K-fold, which consists on splitting the training set into K number of subsets, called folds**. Then, we iteratively fit the model K times, each time training the data on K-1 of the folds and evaluating on the Kth fold (called the validation data). So, at the very end of training, we average the performance on each of the folds to come up with final validation metrics for the model.\n",
    "\n",
    "In the following picture, we can see visually how cross-validation works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"FOLD_CROSS-VALIDATION.PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the K-fold technique works with our random forest regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(estimator = rf_regressor, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(scores.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(scores.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a maximum accuracy of 85.15% applying the K-fold cross-validation technique.\n",
    "\n",
    "Let's check how to improve the result with another popular cross-validation technique, which is called *Grid-Search tecnique*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Applying Grid-Search technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to optimize our random forest model tunign some hyper-parameters to get a better performance. In order to do it, we will first do a random search to review what are the hyper-parameters ranges of values that can fit our model to achieve a good score. Then, we will apply the \"Grid-Search\" method to find the best parameters for our regression model.\n",
    "\n",
    "Before starting tunning the hyper-parameters, we need to check what are the parameters that we are using now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf_regressor.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try adjusting the following set of hyperparameters:\n",
    "* __n_estimators__ = number of trees in the foreset\n",
    "* __max_features__ = max number of features considered for splitting a node\n",
    "* __max_depth__ = max number of levels in each decision tree\n",
    "* __min_samples_split__ = min number of data points placed in a node before the node is split\n",
    "* __min_samples_leaf__ = min number of data points allowed in a leaf node\n",
    "* __bootstrap__ = method for sampling data points (with or without replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.1 Creating a parameter grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start our analysis applying the **Random Hyper-parameters Grid technique**, whose benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values.\n",
    "\n",
    "In order to apply the Random Hyper-parameters Grid technique, we have to use the **RandomizedSearchCV class**. So, we first need to create a parameter grid to sample from during fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a parameter grid\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> On each iteration, the algorithm will choose a difference combination of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2 Random Search Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the random grid to search what are the most powerpul values of the hyper-parameters of the random forest regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the random grid to search for best hyperparameters\n",
    "\n",
    "# First create the base model to tune\n",
    "rf_bm = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "\n",
    "# Random search of parameters, using 10 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf_bm, param_distributions = random_grid, n_iter = 100, cv =5, verbose=2, \n",
    "                               random_state=0, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> The most important arguments in RandomizedSearchCV are n_iter, which controls the number of different combinations to try, and cv which is the number of folds to use for cross validation. In our case, we used a total of 100 iteractions and 5 folds. In addition, we realized that the run time has increased due to the number of folds chosen, but this allows us to reduce the risk of excess.\n",
    "\n",
    "Now, let's check the best parameters from fitting the random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the best random-search parameters\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.3 Evaluate the Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine if random search got a better model, we compute the Score(R2) and MSE metrics of the rf_random model. Then, we compare the results of the random search model with the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (2) for the rf_random and y_training\n",
    "print('Training Score: {}'.format(rf_random.score(X_train, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the rf_random  and y_training\n",
    "print('Training MSE: {}'.format(np.mean((rf_random.predict(X_train) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further improve our results by using grid search to focus on the most promising hyperparameters ranges found in the random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.4 Initiate the grid search model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have are clear about the ranges of values that can fit our model to achieve a good score, we are ready to apply the grid-search technique to find the best parameters for our final predictive regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = [{\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [1.0, 2, 3],\n",
    "    'n_estimators': [1000, 1200, 1400, 1600]}]\n",
    "\n",
    "# Create a based model\n",
    "rf_gs = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf_gs, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.5 Fitting the grid search to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the best grid-search accuracy\n",
    "best_accuracy = grid_search.best_score_\n",
    "#check the best grid-search parameters\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments =>> The obtained results applying the Grid-search method are very similar to the obtained results applying the K-Fold cross-validation technique (86.30 instead of 85.15, respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.6 Fitting the final Random Forest Regressor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we proceed to build and fit the final Random Forest Regressor Model keeping in mind the best parameters values that the grid-search has provided us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the final Random Forest Regressor Model\n",
    "\n",
    "regressor_finalmodel = RandomForestRegressor(n_estimators = 1400, random_state = 0, bootstrap = False, max_depth = 80,\n",
    "                                             max_features = 3, min_samples_leaf = 1, min_samples_split = 2)\n",
    "\n",
    "regressor_finalmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Score (2) for the regressor_finalmodel and y_training\n",
    "print('Training Score: {}'.format(regressor_finalmodel.score(X_train, y_train)))\n",
    "#Compute MSE (Mean Squared Error) for the regressor_finalmodel  and y_training\n",
    "print('Training MSE: {}'.format(np.mean((regressor_finalmodel.predict(X_train) - y_train)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.7 Predicting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[208500.   181500.   223500.   ... 266500.   142125.   147505.54]\n"
     ]
    }
   ],
   "source": [
    "#predict the test results of the train set\n",
    "y_pred_3 = regressor_finalmodel.predict(X_train)\n",
    "np.set_printoptions(precision=2)\n",
    "print(y_pred_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regressor_finalmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8d2378310b9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#predict the test results of the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred_4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregressor_finalmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'regressor_finalmodel' is not defined"
     ]
    }
   ],
   "source": [
    "#predict the test results of the test set\n",
    "y_pred_4 = regressor_finalmodel.predict(df_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print(y_pred_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       208500.0\n",
      "1       181500.0\n",
      "2       223500.0\n",
      "3       140000.0\n",
      "4       250000.0\n",
      "5       143000.0\n",
      "6       307000.0\n",
      "7       200000.0\n",
      "8       129900.0\n",
      "9       118000.0\n",
      "10      129500.0\n",
      "11      345000.0\n",
      "12      144000.0\n",
      "13      279500.0\n",
      "14      157000.0\n",
      "15      132000.0\n",
      "16      149000.0\n",
      "17       90000.0\n",
      "18      159000.0\n",
      "19      139000.0\n",
      "20      325300.0\n",
      "21      139400.0\n",
      "22      230000.0\n",
      "23      129900.0\n",
      "24      154000.0\n",
      "25      256300.0\n",
      "26      134800.0\n",
      "27      306000.0\n",
      "28      207500.0\n",
      "29       68500.0\n",
      "30       40000.0\n",
      "31      149350.0\n",
      "32      179900.0\n",
      "33      165500.0\n",
      "34      277500.0\n",
      "35      309000.0\n",
      "36      145000.0\n",
      "37      153000.0\n",
      "38      109000.0\n",
      "39       82000.0\n",
      "40      160000.0\n",
      "41      170000.0\n",
      "42      144000.0\n",
      "43      130250.0\n",
      "44      141000.0\n",
      "45      319900.0\n",
      "46      239686.0\n",
      "47      249700.0\n",
      "48      113000.0\n",
      "49      127000.0\n",
      "50      177000.0\n",
      "51      114500.0\n",
      "52      110000.0\n",
      "53      385000.0\n",
      "54      130000.0\n",
      "55      180500.0\n",
      "56      172500.0\n",
      "57      196500.0\n",
      "58      438780.0\n",
      "59      124900.0\n",
      "60      158000.0\n",
      "61      101000.0\n",
      "62      202500.0\n",
      "63      140000.0\n",
      "64      219500.0\n",
      "65      317000.0\n",
      "66      180000.0\n",
      "67      226000.0\n",
      "68       80000.0\n",
      "69      225000.0\n",
      "70      244000.0\n",
      "71      129500.0\n",
      "72      185000.0\n",
      "73      144900.0\n",
      "74      107400.0\n",
      "75       91000.0\n",
      "76      135750.0\n",
      "77      127000.0\n",
      "78      136500.0\n",
      "79      110000.0\n",
      "80      193500.0\n",
      "81      153500.0\n",
      "82      245000.0\n",
      "83      126500.0\n",
      "84      168500.0\n",
      "85      260000.0\n",
      "86      174000.0\n",
      "87      164500.0\n",
      "88       85000.0\n",
      "89      123600.0\n",
      "90      109900.0\n",
      "91       98600.0\n",
      "92      163500.0\n",
      "93      133900.0\n",
      "94      204750.0\n",
      "95      185000.0\n",
      "96      214000.0\n",
      "97       94750.0\n",
      "98       83000.0\n",
      "99      128950.0\n",
      "100     205000.0\n",
      "101     178000.0\n",
      "102     118964.0\n",
      "103     198900.0\n",
      "104     169500.0\n",
      "105     250000.0\n",
      "106     100000.0\n",
      "107     115000.0\n",
      "108     115000.0\n",
      "109     190000.0\n",
      "110     136900.0\n",
      "111     180000.0\n",
      "112     383970.0\n",
      "113     217000.0\n",
      "114     259500.0\n",
      "115     176000.0\n",
      "116     139000.0\n",
      "117     155000.0\n",
      "118     320000.0\n",
      "119     163990.0\n",
      "120     180000.0\n",
      "121     100000.0\n",
      "122     136000.0\n",
      "123     153900.0\n",
      "124     181000.0\n",
      "          ...   \n",
      "1320    125000.0\n",
      "1321    167900.0\n",
      "1322    135000.0\n",
      "1323     52500.0\n",
      "1324    200000.0\n",
      "1325    128500.0\n",
      "1326    123000.0\n",
      "1327    155000.0\n",
      "1328    228500.0\n",
      "1329    177000.0\n",
      "1330    155835.0\n",
      "1331    108500.0\n",
      "1332    262500.0\n",
      "1333    283463.0\n",
      "1334    215000.0\n",
      "1335    122000.0\n",
      "1336    200000.0\n",
      "1337    171000.0\n",
      "1338    134900.0\n",
      "1339    410000.0\n",
      "1340    235000.0\n",
      "1341    170000.0\n",
      "1342    110000.0\n",
      "1343    149900.0\n",
      "1344    177500.0\n",
      "1345    315000.0\n",
      "1346    189000.0\n",
      "1347    260000.0\n",
      "1348    104900.0\n",
      "1349    156932.0\n",
      "1350    144152.0\n",
      "1351    216000.0\n",
      "1352    193000.0\n",
      "1353    127000.0\n",
      "1354    144000.0\n",
      "1355    232000.0\n",
      "1356    105000.0\n",
      "1357    165500.0\n",
      "1358    274300.0\n",
      "1359    250000.0\n",
      "1360    239000.0\n",
      "1361     91000.0\n",
      "1362    117000.0\n",
      "1363     83000.0\n",
      "1364    167500.0\n",
      "1365     58500.0\n",
      "1366    237500.0\n",
      "1367    157000.0\n",
      "1368    112000.0\n",
      "1369    105000.0\n",
      "1370    125500.0\n",
      "1371    250000.0\n",
      "1372    136000.0\n",
      "1373    377500.0\n",
      "1374    131000.0\n",
      "1375    235000.0\n",
      "1376    124000.0\n",
      "1377    123000.0\n",
      "1378    163000.0\n",
      "1379    246578.0\n",
      "1380    281213.0\n",
      "1381    160000.0\n",
      "1382    137500.0\n",
      "1383    138000.0\n",
      "1384    137450.0\n",
      "1385    120000.0\n",
      "1386    193000.0\n",
      "1387    193879.0\n",
      "1388    282922.0\n",
      "1389    105000.0\n",
      "1390    275000.0\n",
      "1391    133000.0\n",
      "1392    112000.0\n",
      "1393    125500.0\n",
      "1394    215000.0\n",
      "1395    230000.0\n",
      "1396    140000.0\n",
      "1397     90000.0\n",
      "1398    257000.0\n",
      "1399    207000.0\n",
      "1400    175900.0\n",
      "1401    122500.0\n",
      "1402    340000.0\n",
      "1403    124000.0\n",
      "1404    223000.0\n",
      "1405    179900.0\n",
      "1406    127500.0\n",
      "1407    136500.0\n",
      "1408    274970.0\n",
      "1409    144000.0\n",
      "1410    142000.0\n",
      "1411    271000.0\n",
      "1412    140000.0\n",
      "1413    119000.0\n",
      "1414    182900.0\n",
      "1415    192140.0\n",
      "1416    143750.0\n",
      "1417     64500.0\n",
      "1418    186500.0\n",
      "1419    160000.0\n",
      "1420    174000.0\n",
      "1421    120500.0\n",
      "1422    394617.0\n",
      "1423    149700.0\n",
      "1424    197000.0\n",
      "1425    191000.0\n",
      "1426    149300.0\n",
      "1427    310000.0\n",
      "1428    121000.0\n",
      "1429    179600.0\n",
      "1430    129000.0\n",
      "1431    157900.0\n",
      "1432    240000.0\n",
      "1433    112000.0\n",
      "1434     92000.0\n",
      "1435    136000.0\n",
      "1436    287090.0\n",
      "1437    145000.0\n",
      "1438     84500.0\n",
      "1439    185000.0\n",
      "1440    175000.0\n",
      "1441    210000.0\n",
      "1442    266500.0\n",
      "1443    142125.0\n",
      "1444    147500.0\n",
      "Name: SalePrice, Length: 1445, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the y_pred results into dataframes\n",
    "y_pred_3 = pd.DataFrame(y_pred_3)\n",
    "y_pred_4 = pd.DataFrame(y_pred_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the y_pred_3 and y_pred_4 to csv\n",
    "y_pred_3.to_csv('y_pred_rf_train.csv', index=False)\n",
    "y_pred_4.to_csv('y_pred_rf_test.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.8 Computing metrics for the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared = 0.9999959716067569\n",
      "MSE = 19508.861070310475\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "#Compute Score (2) for the y_training and y_pred_3\n",
    "print(\"R-squared =\", metrics.r2_score(y_train, y_pred_3))\n",
    "#Compute MSE (Mean Squared Error) for the y_training and y_pred_3\n",
    "print(\"MSE =\", metrics.mean_squared_error(y_train, y_pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 139.67412455537524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "#Compute Root-Mean-Squared-Error (RMSE) for the y_training and y_pred_3\n",
    "print(\"RMSE =\", sqrt(mean_squared_error(y_train, y_pred_3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations:__ Root-Mean-Squared-Error (RMSE) is always non-negative, and a value of 0 (almost never achieved in practice) would indicate a perfect fit to the data. In general, a lower RMSD is better than a higher one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.9 Create a submission in the Kaggle competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in our analysis is to merge the \"Id\" column of the test set with the predicted results (y_pred_4) in order to make a submission in the Kaggle competition. One of the Kaggle's rules is to upload a file with 1459 prediction rows and a header row with the \"Id\" and \"SalePrice\" columns.\n",
    "\n",
    "So, we are going to compile this information in one dataframe and create a submission in the Kaggle Leaderboard.\n",
    "\n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>176000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>158000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>190000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>215000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>201000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id         0\n",
       "0  1461  176000.0\n",
       "1  1462  158000.0\n",
       "2  1463  190000.0\n",
       "3  1464  215000.0\n",
       "4  1465  201000.0"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the \"Id_test\" dataframe to merge the Id column of the test set with the y_pred_4 results later \n",
    "Id_test = df_test_prelim[['Id']]\n",
    "#Merging the \"Id_test\" set with the y_pred_4_df results \n",
    "rf_final_results = pd.concat([Id_test, y_pred_4], axis=1,)\n",
    "rf_final_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 2)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape of the rf_final_results dataframe\n",
    "rf_final_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'SalePrice'], dtype='object')"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rename the columns of the  rf_final_results dataframe\n",
    "rf_final_results.columns = ['Id', 'SalePrice']\n",
    "rf_final_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the rf_final_results and y_pred_4 to csv\n",
    "rf_final_results.to_csv('rf_final_results.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to finally our analysis, we have collected the summarized the outcomes results and the errors of the prediction models (logistics regression, randome forest and artificial neural network).\n",
    "\n",
    "You can check the evidences in the following link:\n",
    "\n",
    "[Outcomes Models Summary](https://onedrive.live.com/edit.aspx?cid=ed1967779d009305&page=view&resid=ED1967779D009305!383&parId=ED1967779D009305!331&app=PowerPoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__End of analysis.__\n",
    "\n",
    "__Thanks for reading!!__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
